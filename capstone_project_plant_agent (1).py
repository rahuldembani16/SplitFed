# -*- coding: utf-8 -*-
"""Capstone Project Plant Agent

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/rahuldembani/capstone-project-plant-agent.edf5cbb4-568e-4383-b32a-bdcd3cab2ab3.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250807/auto/storage/goog4_request%26X-Goog-Date%3D20250807T072159Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1382007b226cdb17c32b5861afc98cd7b6d3a0d600715aa760f27b4e4e427d50d1197dde498578dfa1146036ec96151d2435f09d110fc9b448899eb2933b8c6dcff61edba32984750afee72dbb3fa304f6ba43be0ea530de09577b60c3f0e3dc8b84e865663f9485b46e94ffd8a30c12b0eccdffdae9ddbde21e52193928fe37bf2aac648c2ebcae09c1d3cffb3ef80833763e2ce18e9fba95f56edef0c97aeb486c37eb92118476d5b9ba19ec65377cdeef4fdd56e8d893d2a4feba943603f6c6ea610f062b7f01fca6769655690997340d1179c2b70fd18d9788cc056ac312f9e678e59c804b13298ca0c92bcbc7f74ca60b249d44a2f97db6f71c49fddaf7
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
vipoooool_new_plant_diseases_dataset_path = kagglehub.dataset_download('vipoooool/new-plant-diseases-dataset')
santiagosangrocid_plantdoc_model_keras_default_1_path = kagglehub.model_download('santiagosangrocid/plantdoc_model/Keras/default/1')

print('Data source import complete.')

"""# üåø Plant Disease Detector using Generative AI üåø

Welcome to our capstone project! In this notebook, we built an AI-powered tool that takes a plant leaf image and:
1. Identifies if the plant is healthy or diseased üå±
2. If diseased, it gives a **friendly, chatbot-style treatment suggestion** ü©∫
3. Uses **Generative AI capabilities** to improve user interaction üß†

This is designed to help plant lovers get fast insights into plant health.


The project was created by Santiago Sangro Cid, Afia Noureen, and Vinicius Beltran.

## üõ† Environment Configuration üöÄ
This section builds the essential Python environment with the necessary tools and installs and imports the Gemini API Python SDK.
"""

# Uninstall packages from Kaggle base image that are not needed.
!pip uninstall -qy jupyterlab jupyterlab-lsp
# Install gradio for chat user interface.
!pip install -qU gradio
# Install the google-genai SDK for this codelab.
!pip install -qU 'google-genai==1.7.0'

# Install gradio for chat user interface.
!pip install -qU gradio

# Install the google-genai SDK for this codelab.
!pip install -qU 'google-genai==1.7.0'

from google import genai
from google.genai import types

from IPython.display import Markdown, HTML, display

genai.__version__

"""## Set up your API key
To run the following cell, your API key must be stored it in a Kaggle secret named GOOGLE_API_KEY.


"""

from kaggle_secrets import UserSecretsClient

GOOGLE_API_KEY = UserSecretsClient().get_secret("GOOGLE_API_KEY")

client = genai.Client(api_key=GOOGLE_API_KEY)

# Define a retry policy. The model might make multiple consecutive calls automatically
# for a complex query, this ensures the client retries if it hits quota limits.
from google.api_core import retry

is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})

if not hasattr(genai.models.Models.generate_content, '__wrapped__'):
  genai.models.Models.generate_content = retry.Retry(
      predicate=is_retriable)(genai.models.Models.generate_content)

"""## üì¶ Library Imports & Environment Setup

We start by importing essential Python libraries for:
- Deep learning using **TensorFlow** and **PyTorch**
- Data loading, image processing, and augmentation
- Visualization with **Matplotlib** and **Seaborn**

This ensures our environment is ready for training and testing the plant disease detection model.

‚ö†Ô∏è *Note on warnings:*

You may see a few initialization warnings related to CUDA or plugin registration (e.g., cuDNN, cuBLAS) in the output logs. These are normal and occur when GPU-accelerated libraries like TensorFlow and PyTorch attempt to register shared CUDA components. They do not affect the functionality or accuracy of the model and are safe to ignore.
"""

import warnings
warnings.filterwarnings('ignore')
import os
import numpy as np
import matplotlib.pyplot as plt
import gradio as gr
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image

"""# üåø Plant Classification Model
In this section, we use a deep learning model called **MobileNetV2**, which is a lightweight and efficient convolutional neural network architecture optimized to classify images. We've fine-tuned this model on the plant dataset displayed below to classify different types of plants effectively.

You're welcome to train your own model from scratch using the dataset, or simply skip the next sections and use the pre-trained model we've already trained and loaded for you in this notebook to get started quickly.

## üöÄ Train your Own Model!
The following sections will guide you on how we trained the model on the  New Plant Diseases Dataset (Augmented).

### üìÇ Dataset Overview

We used the **New Plant Diseases Dataset (Augmented)** from Kaggle:
- üì∏ 38 classes (including healthy plants)
- üß™ Covers crops like Tomato, Apple, Corn, Grape, Potato, etc.
- ‚úÖ Pre-sorted folders for training and validation

Each image is labeled like: `Tomato___Early_blight`.  
We later processed these to extract just the disease name.

### üìÅ Dataset Directory Setup

We define the dataset paths for training and validation:

- `train_dir` points to the training images
- `valid_dir` points to the validation images
- `Diseases_classes` stores the list of all class folders (i.e., plant diseases and healthy categories)

This helps us organize and load the data efficiently for model training.
"""

root_dir = '/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/'

train_dir = root_dir + "/train"
valid_dir = root_dir + "/valid"
Diseases_classes = os.listdir(train_dir)

"""### üñºÔ∏è Visualizing the Dataset

In this section, we:

- Loop through all the plant disease classes in the training set
- Count and print the **number of images** in each class
- Display the **first image** from each class using `matplotlib`

This gives us a quick visual overview of the dataset and helps confirm that the images are loaded correctly.

üìä Total classes visualized: 38  
üîç Useful for identifying class imbalance or unusual samples
"""

plt.figure(figsize=(60,60))
cnt = 0
plant_names = []
tot_images = 0
for i in Diseases_classes:
    cnt += 1
    plant_names.append(i)
    plt.subplot(7,7,cnt)

    image_path = os.listdir(train_dir + "/" + i)
    print("The Number of Images in " +i+ ":", len(image_path), end= " ")
    print("\n")
    tot_images += len(image_path)

    img_show = plt.imread(train_dir + "/" + i + "/" + image_path[0])
    plt.imshow(img_show)
    plt.xlabel(i,fontsize=30)
    plt.xticks([])
    plt.yticks([])

"""### ‚öôÔ∏è Model Configuration Parameters

We define a few key parameters for training:

- `IMAGE_SIZE`: Target size for resizing all input images (255 x 255 pixels)
- `BATCH_SIZE`: Number of images processed at a time during training (16)
- `EPOCHS`: Number of training cycles (initially set to 5 for quick testing)

Feel free to modify these parameters for better performance or faster training.
"""

IMAGE_SIZE = (255, 255)
BATCH_SIZE = 16
EPOCHS = 5

"""### üß™ Data Preprocessing and Augmentation

We use `ImageDataGenerator` to:

- **Augment the training data** with random transformations like:
  - Rotation
  - Zoom
  - Shearing
  - Horizontal flipping
- **Rescale** pixel values of all images to the range [0, 1]

This improves model generalization by simulating new variations of the input images.

- `train_data`: Augmented training data generator
- `valid_data`: Validation data generator with only rescaling
- `num_classes`: Total number of plant disease categories

"""

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    shear_range=0.2,
    horizontal_flip=True
)

valid_datagen = ImageDataGenerator(rescale=1./255)

train_data = train_datagen.flow_from_directory(
    train_dir,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)
valid_data = valid_datagen.flow_from_directory(
    valid_dir,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

num_classes = len(train_data.class_indices)

"""### üß† Model Architecture with Transfer Learning (MobileNetV2)

We build a Convolutional Neural Network using **Transfer Learning**:

- **Base Model**: `MobileNetV2` pre-trained on ImageNet (used without its top layer)
  - `include_top=False`: Removes the original classification head
  - `trainable=False`: Freezes all layers to retain learned features

We then add custom layers on top:
- `GlobalAveragePooling2D`: Reduces the spatial dimensions
- `Dropout(0.3)`: Helps prevent overfitting
- `Dense` with `softmax`: Final output layer with neurons equal to the number of disease classes

This allows us to leverage a powerful pre-trained model while customizing it for plant disease classification.
"""

#This cell is run only to train Model
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(255, 255, 3))
base_model.trainable = False  # Freeze base model layers

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.3)(x)
predictions = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

"""### ‚öôÔ∏è Model Compilation & Training

We compile and train the model with the following settings:

- **Optimizer**: Adam with a learning rate of 0.0001
- **Loss Function**: Categorical cross-entropy for multi-class classification
- **Metrics**: Accuracy to evaluate the model's performance

**Callbacks**:
- `EarlyStopping`: Stops training if the model performance stops improving (patience = 3)
- `ModelCheckpoint`: Saves the best model during training ‚Äî specifically, the one that performs best on the validation data.
In this notebook, we‚Äôre saving the model to the following path:
`"/kaggle/working/best_model.keras", save_best_only=True)`

We train the model for a predefined number of epochs (`EPOCHS`) and evaluate on the validation set.

This step trains the model using augmented data and the validation set for evaluation.

‚ö†Ô∏è *Note on warnings:*

Again, you may see a few initialization warnings both in training and testing related to CUDA or plugin registration (e.g., cuDNN, cuBLAS) in the output logs. These are normal and do not affect the functionality or accuracy of the model and are safe to ignore.
"""

#This cell will be used only to train model
# Compile and train
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

callbacks = [
    EarlyStopping(patience=3, restore_best_weights=True),
    ModelCheckpoint("/kaggle/working/best_model.keras", save_best_only=True)
]

history = model.fit(
    train_data,
    validation_data=valid_data,
    epochs=EPOCHS,
    callbacks=callbacks
)

model.save('/kaggle/working/my_trained_model.keras')"""

"""### üìä Model Evaluation

After training, we evaluate the model‚Äôs performance on the validation set:

- **`model.evaluate`**: Calculates the loss and accuracy on the validation data.
- **Validation Accuracy**: We print the accuracy as a percentage.

This helps us assess how well the model is performing and whether it can generalize to unseen data.

"""

# Model is 93% accurate.
loss, acc = model.evaluate(valid_data)
print(f"Validation Accuracy: {acc * 100:.2f}%")"""

"""## üì• Loading the Pre-trained Model
Now it is time to put our model to the test!
We load the trained model from the saved file:

- **`model_path`**: This should point to the file where your trained model is saved.
  - If you **trained your own model**, use the same path you set in the `ModelCheckpoint` callback.
  - Otherwise, you can use the pre-trained model we've provided.
- **`load_model`**: Loads the pre-trained model for inference or further evaluation.

This step ensures we can use the model after training to make predictions on new plant images.

"""

model_path = '/kaggle/input/plantdoc_model/keras/default/1/plant_doc_model.keras'

# Load the model    /kaggle/input/plantdoc_model/keras/default/1
model = load_model(model_path)

"""## üì∏ Image Loading & Prediction

This section let's you observe how the model classifies the images! It does the following tasks:

1. **Image Loading**:
   - Loads an image from the training directory, currently set to (`Orange___Haunglongbing_(Citrus_greening)`). You can modify the class to any of the classes available in the dataset or the the index of the image in order to test it. It also resizes it to the target size for prediction.
   - Normalizes the image as done during training.

2. **Prediction**:
   - The model predicts the disease class for the image.
   - The predicted class index is mapped to a human-readable disease label using the `class_labels` list.

3. **Display Prediction**:
   - Displays the image and the predicted disease label on the plot.

This helps visualize how the model performs when given a new image.
"""

#image Loading
img_path = os.path.join(train_dir, 'Orange___Haunglongbing_(Citrus_greening)',
                        os.listdir(os.path.join(train_dir, 'Orange___Haunglongbing_(Citrus_greening)'))[100])

img = image.load_img(img_path, target_size=IMAGE_SIZE)
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)  # Make batch of 1
img_array /= 224.0  # Normalize as done during training

# Make prediction
predictions = model.predict(img_array)
#For predictions, select highest class
predicted_class = np.argmax(predictions, axis=1)[0]
# Map class index to class label
class_labels = list(train_data.class_indices.keys())  # reuse from earlier
predicted_label = class_labels[predicted_class]

plt.imshow(img)
plt.title(f"Predicted: {predicted_label}", fontsize=16)
plt.axis('off')
plt.show()

"""# üåø Creating the Plant Doctor Agent ü§ñ

In this section, we bring together the power of image classification and generative AI to create a helpful assistant for plant lovers ‚Äî the **Plant Doctor Agent**! ü™¥

We use our trained **image classifier** (based on MobileNetV2) to identify plant types or potential issues from images üå±. Then, we enhance the experience using **Gemini 2 Flash**, a generative AI model that provides friendly, detailed responses üí¨ ‚Äî whether it's advice on plant care, identifying diseases, or suggesting the best environment for growth üåûüåßÔ∏è.

By combining visual recognition with conversational intelligence, our agent becomes a smart and interactive tool to help users understand and care for their plants better üåº‚ú®.

## ü¶† Extracting Disease Names from Labels

The `extract_disease` function takes a label (e.g., 'Tomato___Late_blight') and:

- Splits it using the delimiter `___`
- Checks if the label indicates a healthy plant and returns "Healthy"
- Otherwise, it extracts the disease name and formats it by replacing underscores with spaces

This function simplifies the task of mapping model outputs to readable disease names.
"""

def extract_disease(label):
    parts = label.split("___")
    if "healthy" in parts[-1].lower():
        return "Healthy"
    disease = parts[-1].replace("_", " ")
    return disease

"""## ü™Ñ Few-Shot Prompting
We use few-shot prompting to guide the model's responses through a few examples of how to answer plant care questions based on our image-based diagnoses. This helps the model understand the desired tone, structure, and level of detail ‚Äî resulting in more accurate, helpful, and context-aware replies for users. üå±‚ú®
"""

few_shot_prompt = """
You are a plant care assistant. Based on the disease diagnosis and user question, give a friendly, accurate, and helpful response. Always begin by stating what the diagnosis is.

EXAMPLE:
Diagnosis: Healthy
User Question: How often should I water it?
Answer: Your plant is healthy, great job! üå± For most healthy plants, watering once a week is enough, but always check the soil moisture.

EXAMPLE:
Diagnosis: Powdery Mildew
User Question: What should I do now?
Answer: Powdery Mildew is a fungal infection. üåø Remove affected leaves and avoid watering from above. Neem oil or a mild fungicide can help treat it.

EXAMPLE:
Diagnosis: Leaf Spot
User Question: Is it dangerous to other plants?
Answer: Leaf Spot can spread if not treated. üçÇ Keep the infected plant isolated and remove affected leaves. Maintain good air circulation and avoid water splashes.

NEW CASE:
"""

"""## üîß Helper Functions
In order to put it all together, we have separated the different tasks into functions:

1. **Image Preprocessing**:
   - The image is resized to match the input shape of the model, normalized, and reshaped into a batch of 1.

2. **Diagnosis**:
   - The `diagnose_plant` function processes the image, makes a prediction using the model, and extracts the disease name.
   - The corresponding treatment advice is then retrieved and displayed along with the diagnosis.
3. **Model Call**
   - This function takes as an input the disease, and the text prompt introduced by the user. Then, we have enabled search grounding, specified as a tool: google_search.
"""

# Get class labels (used in training)
class_labels = list(train_data.class_indices.keys())

# Image preprocessing function
def preprocess_image(img):
    img = img.resize((255, 255))  # Change to your model‚Äôs input size
    img = np.array(img) / 255.0 #Normalize the Image
    img = np.expand_dims(img, axis=0)
    return img


def diagnose_plant(image):
    img = preprocess_image(image)
    prediction = model.predict(img)
    predicted_class = class_labels[np.argmax(prediction)]

    disease_name = extract_disease(predicted_class)
    return disease_name


def model_call(text, disease):
    user_case = f"""
    Diagnosis: {disease}
    User Question: {text}
    """
    config_with_search = types.GenerateContentConfig(
    tools=[types.Tool(google_search=types.GoogleSearch())],
    )
    response = client.models.generate_content(
            model='gemini-2.0-flash',
            contents=[few_shot_prompt, user_case],
            config=config_with_search,
    )
    full_response = ""
    for candidate in response.candidates:
        for part in candidate.content.parts:
            full_response += part.text + "\n"

    return full_response

"""## ü§ñ Gradio Interface for Plant Disease Diagnosis
In this section, we create a Gradio interface that allows users to interact with the model:

4. **Gradio Interface**:
   - A user-friendly interface is created with `gr.Interface()`, where users can upload an image, and the system will return the diagnosis and advice.

5. **Launching the App**:
   - The app is launched with the `.launch()` method, allowing users to interact with it directly.

This step turns your model into an interactive tool for users to get real-time plant health advice.
"""

def process_inputs(image, text):
    disease = diagnose_plant(image)
    response = model_call(text, disease)
    return response

# Create Gradio chatbot interface
chatbot = gr.Interface(
    fn=process_inputs,
    inputs=[gr.Image(type="pil"), gr.Textbox()],
    outputs=gr.Markdown(label="Plant Doctor Response"),
    title="üåø Plant Disease Diagnosis Bot",
    description="Upload a plant leaf image and get the disease diagnosis with treatment advice."
)

# Launch the app
chatbot.launch(share=True)

"""## üß† Generative AI Capabilities Demonstrated

We used the following GenAI features:

1. **Interactive Image-Based Chatbot Agent** ‚Äì responds to images in a friendly, human-like manner.
2. **Image understanding**
3. **Customized Disease-to-Advice Generation**
4. **Few-shot prompting**
5. **Google Search grounding**
6. **Future-ready modular design** ‚Äì Possibility to be extended to multi-language or voice-based interaction.


## ‚ö†Ô∏è Limitations & Future Improvements

- The current model is limited to recognizing only the plant diseases included in the dataset.
- It does not currently identify the plant species.
- Diagnosis is based on a single image and may be affected by lighting or image quality.

### üîÆ Future Plans:
- Develop a **Mixture of Experts** model to identify not only plant diseases but also plant species, allowing for more tailored advice. üå±
- Integrate **voice input/output** for improved accessibility and ease of use, especially for users in the field. üé§üîä
- Enhance image recognition capabilities to assess the **severity or extent** of the disease for better treatment recommendations. üì∑üß†
- Expand the dataset to include more plant types and rare diseases for broader coverage. üåç

## ‚úÖ Conclusion

We successfully built a working AI assistant that helps identify plant diseases and offers solutions using an image input.

Thanks for exploring our project! üå±‚ú®  
Feel free to test the Gradio app, and check out our [YouTube demo](#) and blog (link)!

"""